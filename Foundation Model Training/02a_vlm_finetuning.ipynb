{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2255615c5b324c3984adc4edc6f5235f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_cec3d4433f334552ba7776d8db9257ed"
          }
        },
        "f6a76bd17a3448219c4a54327d815879": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00ff63481ce5435ba7788b8023b31fea",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3fc6db9b03e245759e163b7befae0294",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "08231035d9e54ec785b950a466968e38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_7dfd90d9876a4b339c8beea2591e4c66",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_804b795414064895a7d69f6d9ec36304",
            "value": ""
          }
        },
        "01da6fe9cf164099ab6c599b37c29fa1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_a046845d75ee4aa19b955393415c5bfa",
            "style": "IPY_MODEL_3c9405e98bb34ef48032c7ea6d58b2a5",
            "value": true
          }
        },
        "27a324152b15471880f23dd4322a0964": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_38b5a3d7a0d148958ba851ed500d1acb",
            "style": "IPY_MODEL_30b7315dc2704950a72d283851c7eb62",
            "tooltip": ""
          }
        },
        "e3131a5c9284427c86d9d5be8f739a75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e02ae8641264fdfb5823a5f9de250e3",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_7839af106c2e4e0284296864f2ee9baf",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "cec3d4433f334552ba7776d8db9257ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "00ff63481ce5435ba7788b8023b31fea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3fc6db9b03e245759e163b7befae0294": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7dfd90d9876a4b339c8beea2591e4c66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "804b795414064895a7d69f6d9ec36304": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a046845d75ee4aa19b955393415c5bfa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c9405e98bb34ef48032c7ea6d58b2a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "38b5a3d7a0d148958ba851ed500d1acb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30b7315dc2704950a72d283851c7eb62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "8e02ae8641264fdfb5823a5f9de250e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7839af106c2e4e0284296864f2ee9baf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "353b311788bd436990ece5219f39c919": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ae3ed9dfdd24ee0bf3162b0be618cc0",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f4eae9f331cb4dedb60e751b1f3d3aa9",
            "value": "Connecting..."
          }
        },
        "3ae3ed9dfdd24ee0bf3162b0be618cc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4eae9f331cb4dedb60e751b1f3d3aa9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a159efc400443a7aaa7ef218ac03488": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4a2b213841204af1b690d26350af95e2",
              "IPY_MODEL_93645628c9dd4d4ba992f277f3a6f147",
              "IPY_MODEL_0107b0a44589429283c17deb33f2a623"
            ],
            "layout": "IPY_MODEL_0b75781d5fe940949cd688bd2efc947f"
          }
        },
        "4a2b213841204af1b690d26350af95e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03cbb3da3a5d42ddb0defd5df6bb3f91",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3f99abdab2bf4ba294d5ce2662b4f7a9",
            "value": "Processing:â€‡100%"
          }
        },
        "93645628c9dd4d4ba992f277f3a6f147": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4969675779742758e9d12755425781a",
            "max": 45,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ba8c462db6d84b86935ed1644a3e01b5",
            "value": 45
          }
        },
        "0107b0a44589429283c17deb33f2a623": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23908c347b8a4aa79b7dd3cceb4be074",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5aee2f0c9f2d463fbd7adc47986a8262",
            "value": "â€‡45/45â€‡[00:07&lt;00:00,â€‡â€‡2.49it/s]"
          }
        },
        "0b75781d5fe940949cd688bd2efc947f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03cbb3da3a5d42ddb0defd5df6bb3f91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f99abdab2bf4ba294d5ce2662b4f7a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e4969675779742758e9d12755425781a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba8c462db6d84b86935ed1644a3e01b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "23908c347b8a4aa79b7dd3cceb4be074": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5aee2f0c9f2d463fbd7adc47986a8262": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ”® VLM Fine-tuning with LoRA\n",
        "\n",
        "## Visual Instruction Tuning for Physical AI\n",
        "\n",
        "This tutorial demonstrates how to fine-tune Vision Language Models using\n",
        "parameter-efficient methods (LoRA). VLMs are the perception backbone of\n",
        "Physical AI systemsâ€”enabling robots to understand scenes, identify objects,\n",
        "and respond to natural language queries about their environment.\n",
        "\n",
        "### Learning Objectives\n",
        "1. Load and configure a VLM for fine-tuning\n",
        "2. Prepare visual instruction tuning data\n",
        "3. Apply LoRA for efficient training\n",
        "4. Evaluate VLM performance\n",
        "\n",
        "### Use Cases in Physical AI\n",
        "- Scene understanding (\"What objects are on the table?\")\n",
        "- Object identification (\"Where is the red cube?\")\n",
        "- Safety assessment (\"Is the path clear?\")\n",
        "- Human detection (\"Is anyone in the workspace?\")\n",
        "\n",
        "### Prerequisites\n",
        "- GPU with 16GB+ memory (T4, A100)\n",
        "- Hugging Face account for model access\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "G5JfsdifbOhC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Environment Setup"
      ],
      "metadata": {
        "id": "g1WjEgFOcEO0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2Z8ZZgbawvJ",
        "outputId": "53826c7a-fa26-41fe-b726-6544c45ef548"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Packages installed successfully!\n",
            "PyTorch version: 2.9.0+cu126\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n",
            "GPU Memory: 15.83 GB\n"
          ]
        }
      ],
      "source": [
        "!pip install -q torch torchvision\n",
        "!pip install -q transformers>=4.40.0\n",
        "!pip install -q accelerate>=0.27.0\n",
        "!pip install -q bitsandbytes>=0.42.0\n",
        "!pip install -q peft>=0.10.0\n",
        "!pip install -q datasets>=2.18.0\n",
        "!pip install -q trl>=0.8.0\n",
        "!pip install -q pillow\n",
        "!pip install -q wandb\n",
        "!pip install -q evaluate\n",
        "\n",
        "print(\"âœ… Packages installed successfully!\")\n",
        "\n",
        "# %%\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from typing import Dict, List, Optional\n",
        "from dataclasses import dataclass\n",
        "import json\n",
        "\n",
        "from transformers import (\n",
        "    AutoProcessor,\n",
        "    AutoModelForCausalLM,  # Florence-2 uses CausalLM, not Vision2Seq\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "\n",
        "# BitsAndBytesConfig may not be available without bitsandbytes\n",
        "try:\n",
        "    from transformers import BitsAndBytesConfig\n",
        "except ImportError:\n",
        "    BitsAndBytesConfig = None\n",
        "    print(\"âš ï¸ BitsAndBytesConfig not available - quantization disabled\")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from datasets import load_dataset, Dataset\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Tutorial Configuration\n",
        "\n",
        "Choose your tutorial speed based on available time and compute."
      ],
      "metadata": {
        "id": "1yD38Wu_uh9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# TUTORIAL MODE CONFIGURATION\n",
        "# ============================================================\n",
        "# Change this to control dataset size and training time\n",
        "\n",
        "TUTORIAL_MODE = \"fast\"  # Options: \"fast\", \"standard\", \"full\"\n",
        "\n",
        "MODE_CONFIGS = {\n",
        "    \"fast\": {\n",
        "        \"num_samples\": 50,      # Very small for quick demo\n",
        "        \"num_epochs\": 1,\n",
        "        \"eval_samples\": 5,\n",
        "        \"description\": \"Quick demo (~3 min on T4)\",\n",
        "    },\n",
        "    \"standard\": {\n",
        "        \"num_samples\": 200,     # Moderate size, fits in Colab RAM\n",
        "        \"num_epochs\": 2,\n",
        "        \"eval_samples\": 20,\n",
        "        \"description\": \"Meaningful training (~15 min on T4)\",\n",
        "    },\n",
        "    \"full\": {\n",
        "        \"num_samples\": 500,     # Reduced to fit Colab high-RAM\n",
        "        \"num_epochs\": 3,\n",
        "        \"eval_samples\": 50,\n",
        "        \"description\": \"Full training (~45 min on T4)\",\n",
        "    },\n",
        "}\n",
        "\n",
        "config = MODE_CONFIGS[TUTORIAL_MODE]\n",
        "\n",
        "print(f\"\"\"\n",
        "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "â•‘  TUTORIAL CONFIGURATION                                        â•‘\n",
        "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
        "â•‘  Mode:           {TUTORIAL_MODE:>10}                                    â•‘\n",
        "â•‘  Training samples: {config['num_samples']:>7}                                    â•‘\n",
        "â•‘  Epochs:         {config['num_epochs']:>10}                                    â•‘\n",
        "â•‘  Eval samples:   {config['eval_samples']:>10}                                    â•‘\n",
        "â•‘  Est. Time:      {config['description']:>25}         â•‘\n",
        "â•‘                                                                â•‘\n",
        "â•‘  ğŸ’¡ Change TUTORIAL_MODE above for different speeds            â•‘\n",
        "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcKmyeqDcHWv",
        "outputId": "33c6d827-a6c8-4911-d4f4-650fb58a4fdc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
            "â•‘  TUTORIAL CONFIGURATION                                        â•‘\n",
            "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
            "â•‘  Mode:                 fast                                    â•‘\n",
            "â•‘  Training samples:      50                                    â•‘\n",
            "â•‘  Epochs:                  1                                    â•‘\n",
            "â•‘  Eval samples:            5                                    â•‘\n",
            "â•‘  Est. Time:      Quick demo (~3 min on T4)         â•‘\n",
            "â•‘                                                                â•‘\n",
            "â•‘  ğŸ’¡ Change TUTORIAL_MODE above for different speeds            â•‘\n",
            "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Hugging Face Authentication"
      ],
      "metadata": {
        "id": "cLcGRa7Su37U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Uncomment to login (required for some models)\n",
        "login()\n",
        "\n",
        "print(\"ğŸ’¡ Run login() if you need access to gated models\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34,
          "referenced_widgets": [
            "2255615c5b324c3984adc4edc6f5235f",
            "f6a76bd17a3448219c4a54327d815879",
            "08231035d9e54ec785b950a466968e38",
            "01da6fe9cf164099ab6c599b37c29fa1",
            "27a324152b15471880f23dd4322a0964",
            "e3131a5c9284427c86d9d5be8f739a75",
            "cec3d4433f334552ba7776d8db9257ed",
            "00ff63481ce5435ba7788b8023b31fea",
            "3fc6db9b03e245759e163b7befae0294",
            "7dfd90d9876a4b339c8beea2591e4c66",
            "804b795414064895a7d69f6d9ec36304",
            "a046845d75ee4aa19b955393415c5bfa",
            "3c9405e98bb34ef48032c7ea6d58b2a5",
            "38b5a3d7a0d148958ba851ed500d1acb",
            "30b7315dc2704950a72d283851c7eb62",
            "8e02ae8641264fdfb5823a5f9de250e3",
            "7839af106c2e4e0284296864f2ee9baf",
            "353b311788bd436990ece5219f39c919",
            "3ae3ed9dfdd24ee0bf3162b0be618cc0",
            "f4eae9f331cb4dedb60e751b1f3d3aa9"
          ]
        },
        "id": "LXAG_Ra3cGzB",
        "outputId": "4f7e2fd8-007a-492e-d105-645d3d97168a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2255615c5b324c3984adc4edc6f5235f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¡ Run login() if you need access to gated models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Load Dataset\n",
        "\n",
        "We'll use the LLaVA-Instruct dataset, which contains visual instruction-following\n",
        "examples. This teaches VLMs to respond to questions about images."
      ],
      "metadata": {
        "id": "m8Ash1TAvB-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ğŸ“¥ Loading dataset...\")\n",
        "\n",
        "# We'll try multiple dataset options in order of preference\n",
        "# Note: Many older HF datasets use deprecated loading scripts\n",
        "# We prioritize datasets using modern Parquet format\n",
        "dataset_name = None\n",
        "full_dataset = None\n",
        "\n",
        "# List of datasets to try (in order of preference)\n",
        "DATASET_OPTIONS = [\n",
        "    # (repo_id, config_name, split, description)\n",
        "    # Beans: Small, reliable dataset of leaf diseases (healthy, angular_leaf_spot, bean_rust)\n",
        "    (\"beans\", None, \"train\", \"Beans Leaf Disease (Reliable & Open)\"),\n",
        "\n",
        "    # Fashion MNIST: Extremely reliable backup\n",
        "    (\"fashion_mnist\", None, \"train\", \"Fashion MNIST (Backup)\"),\n",
        "]\n",
        "\n",
        "for repo_id, config_name, split, description in DATASET_OPTIONS:\n",
        "    try:\n",
        "        print(f\"Trying {description}...\")\n",
        "        if config_name:\n",
        "            full_dataset = load_dataset(repo_id, config_name, split=split)\n",
        "        else:\n",
        "            full_dataset = load_dataset(repo_id, split=split)\n",
        "\n",
        "        # Check if dataset has enough samples\n",
        "        if len(full_dataset) < 50:\n",
        "            print(f\"   Skipping: too few samples ({len(full_dataset)})\")\n",
        "            continue\n",
        "\n",
        "        dataset_name = repo_id.split(\"/\")[-1]\n",
        "        print(f\"âœ… Loaded {dataset_name}\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        print(f\"   Unavailable: {str(e)[:60]}...\")\n",
        "        continue\n",
        "\n",
        "if full_dataset is None:\n",
        "    print(\"\\nâš ï¸ Could not load any external dataset.\")\n",
        "    print(\"   Using synthetic demonstration dataset instead.\")\n",
        "else:\n",
        "    print(f\"   Total samples available: {len(full_dataset)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVJGTGYycGta",
        "outputId": "70346a4e-abac-46c3-b87b-6f7fe495c012"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¥ Loading dataset...\n",
            "Trying Beans Leaf Disease (Reliable & Open)...\n",
            "âœ… Loaded beans\n",
            "   Total samples available: 1034\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Subset the data based on tutorial mode\n",
        "if full_dataset is not None:\n",
        "    num_samples = min(config[\"num_samples\"], len(full_dataset))\n",
        "    dataset = full_dataset.shuffle(seed=42).select(range(num_samples))\n",
        "\n",
        "    # Create train/eval split\n",
        "    split = dataset.train_test_split(test_size=0.1, seed=42)\n",
        "    train_dataset = split[\"train\"]\n",
        "    eval_dataset = split[\"test\"].select(range(min(config[\"eval_samples\"], len(split[\"test\"]))))\n",
        "\n",
        "    print(f\"âœ… Using {len(train_dataset)} training samples from {dataset_name}\")\n",
        "    print(f\"âœ… Using {len(eval_dataset)} evaluation samples\")\n",
        "    USE_REAL_DATASET = True\n",
        "else:\n",
        "    print(\"âš ï¸ No external dataset loaded - will use synthetic demo data\")\n",
        "    train_dataset = None\n",
        "    eval_dataset = None\n",
        "    USE_REAL_DATASET = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lJ3QT2JcGmm",
        "outputId": "ed6780e8-1891-4ff4-ab83-4a51c2e8c3cf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Using 45 training samples from beans\n",
            "âœ… Using 5 evaluation samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect dataset structure\n",
        "if USE_REAL_DATASET and train_dataset is not None:\n",
        "    print(\"\\nğŸ“‹ Dataset structure:\")\n",
        "    print(f\"   Columns: {train_dataset.column_names}\")\n",
        "    print(f\"\\n   Sample content:\")\n",
        "    sample = train_dataset[0]\n",
        "    for key, value in sample.items():\n",
        "        if isinstance(value, str):\n",
        "            display_val = value[:80] + \"...\" if len(value) > 80 else value\n",
        "            print(f\"     {key}: \\\"{display_val}\\\"\")\n",
        "        elif isinstance(value, Image.Image):\n",
        "            print(f\"     {key}: PIL Image {value.size}\")\n",
        "        elif isinstance(value, (list, dict)):\n",
        "            print(f\"     {key}: {type(value).__name__} with {len(value)} items\")\n",
        "        else:\n",
        "            print(f\"     {key}: {type(value).__name__}\")\n",
        "else:\n",
        "    print(\"\\nğŸ“‹ Will use synthetic demo dataset (created in Section 8)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BejeC7ptcGdw",
        "outputId": "b1d2f3d0-aa99-44ab-ddd6-36993005fba5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ“‹ Dataset structure:\n",
            "   Columns: ['image_file_path', 'image', 'labels']\n",
            "\n",
            "   Sample content:\n",
            "     image_file_path: \"/home/albert/.cache/huggingface/datasets/downloads/extracted/967f0d9f61a7a8de588...\"\n",
            "     image: PIL Image (500, 500)\n",
            "     labels: int\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Load Model\n",
        "\n",
        "We'll use Florence-2 as our base VLMâ€”it's open, efficient, and works well\n",
        "for fine-tuning. For larger scale, you could use PaliGemma or LLaVA."
      ],
      "metadata": {
        "id": "0qtUoEhVve-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ğŸ”® Loading Vision Language Model...\")\n",
        "\n",
        "# Model selection\n",
        "MODEL_ID = \"microsoft/Florence-2-base\"  # Lightweight, open\n",
        "\n",
        "# Note: For better quality, you could use:\n",
        "# MODEL_ID = \"google/paligemma-3b-pt-224\"  # Requires login\n",
        "# MODEL_ID = \"llava-hf/llava-1.5-7b-hf\"    # Larger, better"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0X_k4fyu9aV",
        "outputId": "d3c3a4e9-a397-4742-da06-9248a710ec19"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”® Loading Vision Language Model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure quantization\n",
        "# Florence-2-base is small (0.23B params), so we can often run it in FP16/BF16 on T4 (16GB)\n",
        "# This avoids quantization-related errors (\"GET was unable to find an engine...\")\n",
        "USE_QUANTIZATION = False  # Set to True if OOM occurs\n",
        "\n",
        "bnb_config = None\n",
        "if USE_QUANTIZATION and torch.cuda.is_available() and BitsAndBytesConfig is not None:\n",
        "    try:\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "        )\n",
        "        print(\"âœ… 4-bit quantization enabled\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Quantization not available: {e}\")\n",
        "        bnb_config = None\n",
        "else:\n",
        "    print(\"â„¹ï¸ Quantization disabled (running in full precision for stability)\")\n",
        "\n",
        "# Load processor and model\n",
        "print(f\"Loading {MODEL_ID}...\")\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "\n",
        "# Note: Florence-2 uses AutoModelForCausalLM\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32, # Use float16 for better compatibility\n",
        "    attn_implementation=\"eager\",\n",
        ")\n",
        "\n",
        "print(f\"âœ… Model loaded!\")\n",
        "print(f\"   Total parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yznrepW1u9UR",
        "outputId": "86ee4bda-d1d8-48d1-cdc5-31ccf1a942c2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â„¹ï¸ Quantization disabled (running in full precision for stability)\n",
            "Loading microsoft/Florence-2-base...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Model loaded!\n",
            "   Total parameters: 231.4M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Configure LoRA\n",
        "\n",
        "LoRA (Low-Rank Adaptation) allows us to fine-tune efficiently by only\n",
        "training a small number of additional parameters."
      ],
      "metadata": {
        "id": "xElSIcxvv2ir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Only prepare for k-bit training if we are actually using quantization\n",
        "if bnb_config is not None:\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# %%\n",
        "# Find target modules for LoRA\n",
        "# Different models have different layer names\n",
        "def find_target_modules(model):\n",
        "    \"\"\"Find linear layers suitable for LoRA.\"\"\"\n",
        "    target_modules = set()\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, torch.nn.Linear):\n",
        "            # Get the layer name (last part)\n",
        "            layer_name = name.split(\".\")[-1]\n",
        "            if layer_name not in [\"lm_head\", \"embed_tokens\"]:\n",
        "                target_modules.add(layer_name)\n",
        "    return list(target_modules)[:8]  # Limit to avoid memory issues\n",
        "\n",
        "target_modules = find_target_modules(model)\n",
        "print(f\"Target modules for LoRA: {target_modules}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nig6jAyWu9Kg",
        "outputId": "a642e7ab-ee2d-4ee2-cad6-ebd882288e14"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target modules for LoRA: ['q_proj', 'fc1', 'v_proj', 'out_proj', 'fc2', 'qkv', 'k_proj', 'proj']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=16,                          # LoRA rank\n",
        "    lora_alpha=32,                 # Scaling factor\n",
        "    lora_dropout=0.05,             # Regularization\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=target_modules,\n",
        ")\n",
        "\n",
        "# Apply LoRA\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Print trainable parameters\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "print(f\"\"\"\n",
        "LoRA Configuration:\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "  Rank (r):           {lora_config.r}\n",
        "  Alpha:              {lora_config.lora_alpha}\n",
        "  Effective scaling:  {lora_config.lora_alpha / lora_config.r}\n",
        "  Dropout:            {lora_config.lora_dropout}\n",
        "  Target modules:     {len(target_modules)} layer types\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fdi-CaRvblm",
        "outputId": "02b51ebc-3bf6-49b2-e816-5b6aa3312ea2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 6,486,320 || all params: 237,900,336 || trainable%: 2.7265\n",
            "\n",
            "LoRA Configuration:\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "  Rank (r):           16\n",
            "  Alpha:              32\n",
            "  Effective scaling:  2.0\n",
            "  Dropout:            0.05\n",
            "  Target modules:     8 layer types\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Data Preprocessing\n",
        "\n",
        "Prepare the data for training by formatting prompts and encoding images."
      ],
      "metadata": {
        "id": "sEvRd37swQLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "    \"\"\"\n",
        "    Preprocess examples for VLM training.\n",
        "\n",
        "    Handles different dataset formats automatically.\n",
        "    \"\"\"\n",
        "    images = []\n",
        "    texts = []\n",
        "\n",
        "    # Determine number of examples\n",
        "    first_key = list(examples.keys())[0]\n",
        "    num_examples = len(examples[first_key])\n",
        "\n",
        "    for i in range(num_examples):\n",
        "        img = None\n",
        "        text = \"\"\n",
        "\n",
        "        # === Get image from various possible columns ===\n",
        "        for img_col in [\"image\", \"img\", \"pixel_values\"]:\n",
        "            if img_col in examples and examples[img_col][i] is not None:\n",
        "                img = examples[img_col][i]\n",
        "                if isinstance(img, Image.Image):\n",
        "                    img = img.convert(\"RGB\")\n",
        "                elif isinstance(img, dict) and \"bytes\" in img:\n",
        "                    from io import BytesIO\n",
        "                    img = Image.open(BytesIO(img[\"bytes\"])).convert(\"RGB\")\n",
        "                break\n",
        "\n",
        "        if img is None:\n",
        "            continue  # Skip samples without valid images\n",
        "\n",
        "        images.append(img)\n",
        "\n",
        "        # === Get text from various possible columns ===\n",
        "        # Caption-style datasets\n",
        "        if \"text\" in examples:\n",
        "            caption = examples[\"text\"][i]\n",
        "            text = f\"Describe this image.\\nAnswer: {caption}\"\n",
        "        elif \"caption\" in examples:\n",
        "            caption = examples[\"caption\"][i]\n",
        "            text = f\"Describe this image.\\nAnswer: {caption}\"\n",
        "        # VQA-style datasets\n",
        "        elif \"question\" in examples:\n",
        "            q = examples[\"question\"][i]\n",
        "            a = examples.get(\"answer\", examples.get(\"label\", [\"\"]))[i]\n",
        "            text = f\"Question: {q}\\nAnswer: {a}\"\n",
        "        # Explanation datasets (e.g., newyorker)\n",
        "        elif \"explanation\" in examples:\n",
        "            expl = examples[\"explanation\"][i]\n",
        "            text = f\"Explain what's happening in this image.\\nAnswer: {expl}\"\n",
        "        # Object detection / safety datasets\n",
        "        elif \"objects\" in examples:\n",
        "            obj_info = examples[\"objects\"][i]\n",
        "            text = f\"What objects are visible?\\nAnswer: Various objects detected in the scene.\"\n",
        "        # Fallback\n",
        "        else:\n",
        "            # Try to find any text column\n",
        "            for col in examples.keys():\n",
        "                if col != \"image\" and isinstance(examples[col][i], str):\n",
        "                    text = f\"Describe this image.\\nAnswer: {examples[col][i]}\"\n",
        "                    break\n",
        "            if not text:\n",
        "                text = \"Describe this image.\\nAnswer: This is an image.\"\n",
        "\n",
        "        texts.append(text)\n",
        "\n",
        "    # Filter out None images\n",
        "    valid_pairs = [(img, txt) for img, txt in zip(images, texts) if img is not None]\n",
        "\n",
        "    if not valid_pairs:\n",
        "        return {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n",
        "\n",
        "    images, texts = zip(*valid_pairs)\n",
        "\n",
        "    # Process with model's processor\n",
        "    try:\n",
        "        encoding = processor(\n",
        "            images=list(images),\n",
        "            text=list(texts),\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "        )\n",
        "\n",
        "        # Create labels\n",
        "        labels = encoding[\"input_ids\"].clone()\n",
        "        labels[labels == processor.tokenizer.pad_token_id] = -100\n",
        "        encoding[\"labels\"] = labels\n",
        "\n",
        "        return {k: v.tolist() for k, v in encoding.items()}\n",
        "    except Exception as e:\n",
        "        print(f\"Preprocessing error: {e}\")\n",
        "        return {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n",
        "\n",
        "\n",
        "print(\"âœ… Preprocessing function defined\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-glq4oCvbb_",
        "outputId": "f29e730d-0cff-4232-b815-88ade6ff5b3f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Preprocessing function defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple data collator with padding support\n",
        "@dataclass\n",
        "class VLMDataCollator:\n",
        "    \"\"\"Collate examples for VLM training with proper padding.\"\"\"\n",
        "    processor: AutoProcessor\n",
        "    pad_token_id: int = 0\n",
        "\n",
        "    def __call__(self, features: List[Dict]) -> Dict[str, torch.Tensor]:\n",
        "        if not features:\n",
        "            return {}\n",
        "\n",
        "        batch = {}\n",
        "\n",
        "        for key in features[0].keys():\n",
        "            if key not in [\"input_ids\", \"attention_mask\", \"labels\", \"pixel_values\"]:\n",
        "                continue\n",
        "\n",
        "            values = [f[key] for f in features if key in f and f[key] is not None]\n",
        "            if not values:\n",
        "                continue\n",
        "\n",
        "            # Convert to tensors\n",
        "            tensors = [torch.tensor(v) if not isinstance(v, torch.Tensor) else v for v in values]\n",
        "\n",
        "            if key == \"pixel_values\":\n",
        "                # Pixel values should be same size, just stack\n",
        "                try:\n",
        "                    batch[key] = torch.stack(tensors)\n",
        "                except:\n",
        "                    # If stacking fails, skip pixel values\n",
        "                    pass\n",
        "            else:\n",
        "                # Pad sequences to same length\n",
        "                max_len = max(t.shape[0] for t in tensors)\n",
        "                padded = []\n",
        "                for t in tensors:\n",
        "                    if t.shape[0] < max_len:\n",
        "                        pad_size = max_len - t.shape[0]\n",
        "                        pad_value = -100 if key == \"labels\" else self.pad_token_id\n",
        "                        t = torch.nn.functional.pad(t, (0, pad_size), value=pad_value)\n",
        "                    padded.append(t)\n",
        "                batch[key] = torch.stack(padded)\n",
        "\n",
        "        return batch\n",
        "\n",
        "\n",
        "# Get pad token id safely\n",
        "try:\n",
        "    pad_token_id = processor.tokenizer.pad_token_id or 0\n",
        "except:\n",
        "    pad_token_id = 0\n",
        "\n",
        "data_collator = VLMDataCollator(processor=processor, pad_token_id=pad_token_id)\n",
        "\n",
        "\n",
        "data_collator = VLMDataCollator(processor=processor)"
      ],
      "metadata": {
        "id": "zZGjI-Yevzdw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Alternative: Use Pre-formatted Dataset\n",
        "\n",
        "For simpler setup, we can create a formatted dataset directly."
      ],
      "metadata": {
        "id": "BUBdu5F1wfHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a simple training dataset with VQA-style examples\n",
        "def create_vqa_dataset(num_samples: int = 100):\n",
        "    \"\"\"\n",
        "    Create a simple VQA dataset for demonstration.\n",
        "\n",
        "    In practice, you would use real datasets like:\n",
        "    - LLaVA-Instruct-150K\n",
        "    - GQA\n",
        "    - VQAv2\n",
        "    - Custom domain data\n",
        "    \"\"\"\n",
        "\n",
        "    # VQA templates relevant to robotics/physical AI\n",
        "    templates = [\n",
        "        (\"What objects are visible in this image?\",\n",
        "         \"I can see various objects in the scene including items on surfaces.\"),\n",
        "        (\"Describe the scene.\",\n",
        "         \"This image shows an indoor environment with objects arranged in the space.\"),\n",
        "        (\"What is the main subject of this image?\",\n",
        "         \"The main subject appears to be the central object or area of focus.\"),\n",
        "        (\"Are there any people in this image?\",\n",
        "         \"I would need to analyze the image to determine if people are present.\"),\n",
        "        (\"What colors are prominent?\",\n",
        "         \"The image contains various colors depending on the objects present.\"),\n",
        "    ]\n",
        "\n",
        "    samples = []\n",
        "    for i in range(num_samples):\n",
        "        template = templates[i % len(templates)]\n",
        "\n",
        "        # Create a simple colored image\n",
        "        color = (\n",
        "            np.random.randint(50, 200),\n",
        "            np.random.randint(50, 200),\n",
        "            np.random.randint(50, 200),\n",
        "        )\n",
        "        image = Image.new(\"RGB\", (224, 224), color=color)\n",
        "\n",
        "        samples.append({\n",
        "            \"image\": image,\n",
        "            \"question\": template[0],\n",
        "            \"answer\": template[1],\n",
        "        })\n",
        "\n",
        "    return Dataset.from_list(samples)\n",
        "\n",
        "\n",
        "# Create simple dataset for demo\n",
        "print(\"Creating demonstration dataset...\")\n",
        "demo_train = create_vqa_dataset(config[\"num_samples\"])\n",
        "demo_eval = create_vqa_dataset(config[\"eval_samples\"])\n",
        "\n",
        "print(f\"âœ… Created {len(demo_train)} training, {len(demo_eval)} eval samples\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e49H33FMvzJ2",
        "outputId": "fe2c66db-07bd-49f8-ce56-86d58feda440"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating demonstration dataset...\n",
            "âœ… Created 500 training, 50 eval samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Training Configuration"
      ],
      "metadata": {
        "id": "jFdjoDKiwr2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create output directory\n",
        "output_dir = \"./vlm_finetuned\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "\n",
        "    # Training duration\n",
        "    num_train_epochs=config[\"num_epochs\"],\n",
        "\n",
        "    # Batch size (adjust based on GPU memory)\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=4,  # Effective batch = 2 * 4 = 8\n",
        "\n",
        "    # Learning rate\n",
        "    learning_rate=2e-5,\n",
        "    warmup_ratio=0.1,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "\n",
        "    # Optimization (use paged optimizer only if bitsandbytes available)\n",
        "    optim=\"paged_adamw_8bit\" if bnb_config is not None else \"adamw_torch\",\n",
        "    weight_decay=0.01,\n",
        "    max_grad_norm=1.0,\n",
        "\n",
        "    # Mixed precision\n",
        "    bf16=torch.cuda.is_available() and torch.cuda.is_bf16_supported(),\n",
        "    fp16=torch.cuda.is_available() and not torch.cuda.is_bf16_supported(),\n",
        "\n",
        "    # Logging\n",
        "    logging_steps=10,\n",
        "    logging_first_step=True,\n",
        "    report_to=\"none\",  # Set to \"wandb\" for experiment tracking\n",
        "\n",
        "    # Saving\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,\n",
        "    save_total_limit=2,\n",
        "\n",
        "    # Evaluation\n",
        "    eval_strategy=\"steps\",  # renamed from evaluation_strategy in newer transformers\n",
        "    eval_steps=50,\n",
        "\n",
        "    # Other\n",
        "    remove_unused_columns=False,\n",
        "    dataloader_pin_memory=True,\n",
        "    seed=42,\n",
        "    torch_compile=False, # Explicitly disable compilation to avoid engine errors\n",
        ")\n",
        "\n",
        "print(\"âœ… Training configuration created\")\n",
        "print(f\"   Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "print(f\"   Learning rate: {training_args.learning_rate}\")\n",
        "print(f\"   Epochs: {training_args.num_train_epochs}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gOw_JbawRr_",
        "outputId": "b65e62a2-526f-4c4a-8c91-03ca3231c577"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Training configuration created\n",
            "   Effective batch size: 8\n",
            "   Learning rate: 2e-05\n",
            "   Epochs: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Custom Trainer for VLM"
      ],
      "metadata": {
        "id": "R2UzQP4Hw12h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VLMTrainer(Trainer):\n",
        "    \"\"\"Custom trainer with VLM-specific handling.\"\"\"\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        \"\"\"Compute loss for VLM training.\"\"\"\n",
        "        outputs = model(**inputs)\n",
        "        loss = outputs.loss\n",
        "        return (loss, outputs) if return_outputs else loss"
      ],
      "metadata": {
        "id": "Y4iyPK-OwRio"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare datasets with Florence-2 compatible processing\n",
        "def simple_preprocess(examples):\n",
        "    \"\"\"\n",
        "    Simple preprocessing for demo dataset.\n",
        "    Florence-2 uses task prompts like <CAPTION>, <VQA>, etc.\n",
        "    \"\"\"\n",
        "    processed_examples = []\n",
        "\n",
        "    images = examples[\"image\"]\n",
        "    questions = examples[\"question\"]\n",
        "    answers = examples[\"answer\"]\n",
        "\n",
        "    input_ids_list = []\n",
        "    attention_mask_list = []\n",
        "    pixel_values_list = []\n",
        "    labels_list = []\n",
        "\n",
        "    for img, q, a in zip(images, questions, answers):\n",
        "        # Florence-2 format: use <VQA> task with question\n",
        "        prompt = f\"<VQA>{q}\"\n",
        "        target = a\n",
        "\n",
        "        try:\n",
        "            # Process image and prompt\n",
        "            inputs = processor(\n",
        "                images=img,\n",
        "                text=prompt,\n",
        "                return_tensors=\"pt\",\n",
        "            )\n",
        "\n",
        "            # Get labels from target text\n",
        "            target_encoding = processor.tokenizer(\n",
        "                target,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=\"max_length\",\n",
        "                max_length=64,\n",
        "                truncation=True,\n",
        "            )\n",
        "\n",
        "            input_ids_list.append(inputs[\"input_ids\"].squeeze(0).tolist())\n",
        "            attention_mask_list.append(inputs[\"attention_mask\"].squeeze(0).tolist())\n",
        "            if \"pixel_values\" in inputs:\n",
        "                pixel_values_list.append(inputs[\"pixel_values\"].squeeze(0).tolist())\n",
        "            labels_list.append(target_encoding[\"input_ids\"].squeeze(0).tolist())\n",
        "\n",
        "        except Exception as e:\n",
        "            # Skip problematic samples\n",
        "            continue\n",
        "\n",
        "    if not input_ids_list:\n",
        "        return {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n",
        "\n",
        "    result = {\n",
        "        \"input_ids\": input_ids_list,\n",
        "        \"attention_mask\": attention_mask_list,\n",
        "        \"labels\": labels_list,\n",
        "    }\n",
        "    if pixel_values_list:\n",
        "        result[\"pixel_values\"] = pixel_values_list\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "# Determine which dataset to use\n",
        "print(\"Processing datasets...\")\n",
        "\n",
        "# Use real dataset from Section 4, or demo dataset from Section 8\n",
        "if USE_REAL_DATASET and train_dataset is not None:\n",
        "    source_train = train_dataset\n",
        "    source_eval = eval_dataset\n",
        "    print(f\"Using real dataset: {dataset_name}\")\n",
        "elif 'demo_train' in dir():\n",
        "    source_train = demo_train\n",
        "    source_eval = demo_eval\n",
        "    print(\"Using demo dataset from Section 8\")\n",
        "else:\n",
        "    # Need to create demo dataset here\n",
        "    print(\"Creating demo dataset...\")\n",
        "    source_train = create_vqa_dataset(config[\"num_samples\"])\n",
        "    source_eval = create_vqa_dataset(config[\"eval_samples\"])\n",
        "    print(f\"Created {len(source_train)} training samples\")\n",
        "\n",
        "# Flexible preprocessing that handles different dataset formats\n",
        "def flexible_preprocess(sample):\n",
        "    \"\"\"Process a single sample from various dataset formats.\"\"\"\n",
        "    try:\n",
        "        # Get image\n",
        "        img = None\n",
        "        for col in [\"image\", \"img\", \"pixel_values\"]:\n",
        "            if col in sample and sample[col] is not None:\n",
        "                img = sample[col]\n",
        "                if isinstance(img, Image.Image):\n",
        "                    img = img.convert(\"RGB\")\n",
        "                break\n",
        "\n",
        "        if img is None:\n",
        "            return None\n",
        "\n",
        "        # Get text content\n",
        "        if \"question\" in sample:\n",
        "            prompt = f\"<VQA>{sample['question']}\"\n",
        "            target = sample.get(\"answer\", sample.get(\"label\", \"\"))\n",
        "        elif \"text\" in sample:\n",
        "            prompt = \"<CAPTION>\"\n",
        "            target = sample[\"text\"]\n",
        "        elif \"caption\" in sample:\n",
        "            prompt = \"<CAPTION>\"\n",
        "            target = sample[\"caption\"]\n",
        "        elif \"labels\" in sample or \"label\" in sample:\n",
        "            # Classification dataset (like beans or fashion_mnist)\n",
        "            label = sample.get(\"labels\", sample.get(\"label\"))\n",
        "            # Convert integer label to string description\n",
        "            if isinstance(label, int):\n",
        "                # Simple mapping for beans dataset\n",
        "                if dataset_name == \"beans\":\n",
        "                    labels = {0: \"angular_leaf_spot\", 1: \"bean_rust\", 2: \"healthy\"}\n",
        "                    target = labels.get(label, str(label))\n",
        "                elif dataset_name == \"fashion_mnist\":\n",
        "                    labels = {0: \"T-shirt/top\", 1: \"Trouser\", 2: \"Pullover\", 3: \"Dress\", 4: \"Coat\",\n",
        "                              5: \"Sandal\", 6: \"Shirt\", 7: \"Sneaker\", 8: \"Bag\", 9: \"Ankle boot\"}\n",
        "                    target = labels.get(label, str(label))\n",
        "                else:\n",
        "                    target = str(label)\n",
        "            else:\n",
        "                target = str(label)\n",
        "            prompt = \"<CAPTION>\"\n",
        "        else:\n",
        "            prompt = \"<CAPTION>\"\n",
        "            target = \"An image.\"\n",
        "\n",
        "        # Ensure target is a string\n",
        "        if isinstance(target, list):\n",
        "            target = target[0] if target else \"\"\n",
        "        target = str(target)\n",
        "\n",
        "        # Process with Florence-2\n",
        "        inputs = processor(images=img, text=prompt, return_tensors=\"pt\")\n",
        "        target_enc = processor.tokenizer(\n",
        "            target, return_tensors=\"pt\",\n",
        "            padding=\"max_length\", max_length=64, truncation=True\n",
        "        )\n",
        "\n",
        "        result = {\n",
        "            \"input_ids\": inputs[\"input_ids\"].squeeze(0).tolist(),\n",
        "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0).tolist(),\n",
        "            \"labels\": target_enc[\"input_ids\"].squeeze(0).tolist(),\n",
        "        }\n",
        "        if \"pixel_values\" in inputs:\n",
        "            result[\"pixel_values\"] = inputs[\"pixel_values\"].squeeze(0).tolist()\n",
        "\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "\n",
        "# Process samples one at a time for robustness\n",
        "import gc\n",
        "\n",
        "processed_data = []\n",
        "num_to_process = min(len(source_train), config[\"num_samples\"])\n",
        "\n",
        "print(f\"Processing {num_to_process} samples...\")\n",
        "for i in tqdm(range(num_to_process), desc=\"Processing\"):\n",
        "    result = flexible_preprocess(source_train[i])\n",
        "    if result is not None:\n",
        "        processed_data.append(result)\n",
        "\n",
        "    # Periodic garbage collection to free memory\n",
        "    if i > 0 and i % 50 == 0:\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "if processed_data:\n",
        "    processed_train = Dataset.from_list(processed_data)\n",
        "    # Create eval set from last portion\n",
        "    eval_size = min(len(processed_data) // 10, config[\"eval_samples\"])\n",
        "    processed_eval = Dataset.from_list(processed_data[-eval_size:]) if eval_size > 0 else None\n",
        "    print(f\"âœ… Processed {len(processed_train)} training samples\")\n",
        "\n",
        "    # Free memory\n",
        "    del processed_data\n",
        "    gc.collect()\n",
        "else:\n",
        "    print(\"âŒ Could not process any samples\")\n",
        "    processed_train = None\n",
        "    processed_eval = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "8a159efc400443a7aaa7ef218ac03488",
            "4a2b213841204af1b690d26350af95e2",
            "93645628c9dd4d4ba992f277f3a6f147",
            "0107b0a44589429283c17deb33f2a623",
            "0b75781d5fe940949cd688bd2efc947f",
            "03cbb3da3a5d42ddb0defd5df6bb3f91",
            "3f99abdab2bf4ba294d5ce2662b4f7a9",
            "e4969675779742758e9d12755425781a",
            "ba8c462db6d84b86935ed1644a3e01b5",
            "23908c347b8a4aa79b7dd3cceb4be074",
            "5aee2f0c9f2d463fbd7adc47986a8262"
          ]
        },
        "id": "BoHVeFzTwhCm",
        "outputId": "3fe5fd50-4230-477d-ac6f-52c30d7e2e41"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing datasets...\n",
            "Using real dataset: beans\n",
            "Processing 45 samples...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing:   0%|          | 0/45 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a159efc400443a7aaa7ef218ac03488"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Processed 45 training samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Train the Model"
      ],
      "metadata": {
        "id": "17DJ8Fz_zbHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize trainer (only if we have data)\n",
        "if processed_train is not None and len(processed_train) > 0:\n",
        "\n",
        "    # Check if we have eval data\n",
        "    has_eval = processed_eval is not None and len(processed_eval) > 0\n",
        "\n",
        "    if not has_eval:\n",
        "        print(\"âš ï¸ No evaluation data available - disabling evaluation\")\n",
        "        training_args.eval_strategy = \"no\"\n",
        "        training_args.evaluation_strategy = \"no\" # Update both for compatibility\n",
        "\n",
        "    trainer = VLMTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=processed_train,\n",
        "        eval_dataset=processed_eval if has_eval else None,\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "    print(\"âœ… Trainer initialized\")\n",
        "    print(f\"   Training samples: {len(processed_train)}\")\n",
        "    if has_eval:\n",
        "        print(f\"   Evaluation samples: {len(processed_eval)}\")\n",
        "else:\n",
        "    trainer = None\n",
        "    print(\"âš ï¸ No training data available - skipping trainer initialization\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78ULDmbkwuBx",
        "outputId": "e0e79436-628a-4699-af7b-f958c012cb6f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Trainer initialized\n",
            "   Training samples: 45\n",
            "   Evaluation samples: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train!\n",
        "print(\"=\" * 60)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if trainer is not None:\n",
        "    try:\n",
        "        trainer.train()\n",
        "        print(\"\\nâœ… Training complete!\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâš ï¸ Training stopped: {e}\")\n",
        "        print(\"This may be due to memory constraints or data format issues.\")\n",
        "        print(\"The concepts demonstrated are still valid for real datasets.\")\n",
        "else:\n",
        "    print(\"âš ï¸ Skipping training - no valid dataset available\")\n",
        "    print(\"   The tutorial demonstrates the training setup process.\")\n",
        "    print(\"   With a proper dataset, training would proceed here.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8a2-AI4x1vR",
        "outputId": "b98d3cb9-81b1-41be-93ef-e24c5de1e0cd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "STARTING TRAINING\n",
            "============================================================\n",
            "\n",
            "âš ï¸ Training stopped: GET was unable to find an engine to execute this computation\n",
            "This may be due to memory constraints or data format issues.\n",
            "The concepts demonstrated are still valid for real datasets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12. Inference with Fine-tuned Model"
      ],
      "metadata": {
        "id": "OHrO3lI7zlXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def run_inference(image: Image.Image, question: str) -> str:\n",
        "    \"\"\"Run inference with the fine-tuned model.\"\"\"\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Prepare input\n",
        "    prompt = f\"Question: {question}\\nAnswer:\"\n",
        "    inputs = processor(\n",
        "        images=image,\n",
        "        text=prompt,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(device)\n",
        "\n",
        "    # Generate (use_cache=False and num_beams=1 for Florence-2 compatibility)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=100,\n",
        "        do_sample=False,\n",
        "        num_beams=1,\n",
        "        use_cache=False,\n",
        "    )\n",
        "\n",
        "    # Decode\n",
        "    response = processor.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return response"
      ],
      "metadata": {
        "id": "q0byP5m3x1kB"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test inference\n",
        "print(\"=\" * 60)\n",
        "print(\"TESTING INFERENCE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "test_image = Image.new(\"RGB\", (224, 224), color=(100, 150, 200))\n",
        "test_questions = [\n",
        "    \"What do you see in this image?\",\n",
        "    \"Describe the colors present.\",\n",
        "    \"Is this image suitable for robot navigation?\",\n",
        "]\n",
        "\n",
        "for question in test_questions:\n",
        "    print(f\"\\nğŸ“ Question: {question}\")\n",
        "    try:\n",
        "        response = run_inference(test_image, question)\n",
        "        print(f\"ğŸ¤– Response: {response}\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Inference error: {e}\")"
      ],
      "metadata": {
        "id": "BS1KGOeZzc_p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92aaf943-1e68-45a0-b306-f58beafdbb05"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "TESTING INFERENCE\n",
            "============================================================\n",
            "\n",
            "ğŸ“ Question: What do you see in this image?\n",
            "âš ï¸ Inference error: Input type (float) and bias type (c10::Half) should be the same\n",
            "\n",
            "ğŸ“ Question: Describe the colors present.\n",
            "âš ï¸ Inference error: Input type (float) and bias type (c10::Half) should be the same\n",
            "\n",
            "ğŸ“ Question: Is this image suitable for robot navigation?\n",
            "âš ï¸ Inference error: Input type (float) and bias type (c10::Half) should be the same\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13. Save Model"
      ],
      "metadata": {
        "id": "GzSiejhGzy_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned LoRA adapters\n",
        "save_path = \"./vlm_lora_adapter\"\n",
        "\n",
        "model.save_pretrained(save_path)\n",
        "processor.save_pretrained(save_path)\n",
        "\n",
        "print(f\"âœ… Model saved to {save_path}\")\n",
        "\n",
        "# Check saved files\n",
        "import os\n",
        "saved_files = os.listdir(save_path)\n",
        "print(f\"Saved files: {saved_files[:10]}...\")"
      ],
      "metadata": {
        "id": "3mTDkKWXzc47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RS0SyRiZzvpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dfb3lv-rzvhr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}